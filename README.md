# Knowledge Distillation for Multi-task Learning
This is the implementation of [Knowledge Distillation for Multi-task Learning](https://arxiv.org/pdf/2007.06889.pdf).  
I used NYUv2 dataset and data augmentation.  
## Result
![Screenshot from 2024-09-19 20-40-01](https://github.com/user-attachments/assets/0cfaaa01-0e9b-406d-8d65-9b7306f07cfd)







